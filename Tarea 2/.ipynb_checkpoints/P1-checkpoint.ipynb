{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WooLONcswlR"
   },
   "source": [
    "# 1. Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Cw-nJ0esvOJ",
    "outputId": "2cbecff0-935f-4793-80f1-9912c4d996d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=d005058ea79ce52b3dac68dadf326cf400522f762c5f4b0b2955a9a06d792db7\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OOwNdY7l00n4"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_fR22JutFNT"
   },
   "outputs": [],
   "source": [
    "class PageRank:\n",
    "    def __init__(self, nodes, edges, damping_factor=0.85):\n",
    "        # Inicializa nodos y bordes\n",
    "        self.nodes = SparkContext.getOrCreate().parallelize(nodes).map(lambda node: (node, 1.0/len(nodes)))\n",
    "        self.edges = SparkContext.getOrCreate().parallelize(edges).groupByKey().mapValues(list)\n",
    "        self.damping_factor = damping_factor\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_messages(node):\n",
    "        # Prepara los mensajes a enviar desde un nodo a sus vecinos\n",
    "        node_id, (rank, neighbors) = node\n",
    "        num_neighbors = len(neighbors)\n",
    "        for neighbor in neighbors:\n",
    "            yield (neighbor, rank / num_neighbors)\n",
    "\n",
    "    @staticmethod\n",
    "    def exchange_messages(messages):\n",
    "        # Reduce los mensajes por clave (nodo de destino)\n",
    "        return messages.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_rank(node, damping_factor, num_nodes):\n",
    "        # Actualiza el PageRank de un nodo\n",
    "        node_id, rank = node\n",
    "        updated_rank = (1 - damping_factor) / num_nodes + damping_factor * rank\n",
    "        return (node_id, updated_rank)\n",
    "\n",
    "    def calculate(self, max_iterations=10, convergence_threshold=0.01):\n",
    "        nodes = self.nodes\n",
    "        edges = self.edges\n",
    "        damping_factor = self.damping_factor\n",
    "        num_nodes = self.nodes.count()\n",
    "        for i in range(max_iterations):\n",
    "            prev_nodes = nodes  # Guarda los nodos de la iteración anterior\n",
    "            join_nodes_edges = nodes.join(edges)  # Une los nodos y los bordes por id de nodo\n",
    "            messages = join_nodes_edges.flatMap(PageRank.prepare_messages)  # Prepara y envía mensajes\n",
    "            final_messages = PageRank.exchange_messages(messages)  # Reduce los mensajes por nodo de destino\n",
    "            nodes = final_messages.map(lambda node: PageRank.update_rank(node, damping_factor, num_nodes))  # Actualiza los PageRank\n",
    "\n",
    "            # Calcula la norma L1 de la diferencia en los puntajes de PageRank entre las iteraciones\n",
    "            diff = (nodes.join(prev_nodes)\n",
    "                      .map(lambda node: abs(node[1][0] - node[1][1]))\n",
    "                      .sum())\n",
    "\n",
    "            # Comprueba si la diferencia es menor que el umbral de convergencia\n",
    "            if diff < convergence_threshold:\n",
    "                print(f\"La ejecución terminó por convergencia en {i+1} iteraciones.\")\n",
    "                break  # Si la diferencia es menor que el umbral, se detiene la iteración\n",
    "\n",
    "        else:\n",
    "            print(f\"La ejecución terminó porque se alcanzó el máximo de {max_iterations} iteraciones.\")\n",
    "\n",
    "        self.nodes = nodes\n",
    "        return self.nodes.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SStysdvyujkM",
    "outputId": "dd7bca8e-d002-415e-cea9-f0698f893782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ejecución terminó porque se alcanzó el máximo de 2 iteraciones.\n",
      "Node 1 has PageRank: 0.09859375000000001\n",
      "Node 2 has PageRank: 0.22078124999999998\n",
      "Node 3 has PageRank: 0.18890624999999997\n",
      "Node 4 has PageRank: 0.18890624999999997\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "nodes = [1, 2, 3, 4]\n",
    "edges = [(1, 2), (2, 3), (2, 4), (3, 2), (3, 1)]\n",
    "\n",
    "pagerank = PageRank(nodes, edges)\n",
    "result = pagerank.calculate(max_iterations=2, convergence_threshold=0.001)\n",
    "\n",
    "for node in result:\n",
    "    print(f\"Node {node[0]} has PageRank: {node[1]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
